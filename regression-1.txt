ESEUR Workshop - Regression modeling
====================================
:author:    Derek M. Jones
:copyright: Somebody
:backend:   slidy
:max-width: 45em

Regression modeling
-------------------

{nbsp}

Today's hammer

{nbsp}

<quote>All models are wrong, but some are useful</quote>

We are using a computer

{nbsp}

Main reasons for building a regression model:

* understanding: willingness to trade-off prediction accuracy for
simplicity

* prediction: willingness to trade-off understanding for greater
accuracy

What is regression modeling?
----------------------------

Fits an equation to data

You decide what equations to fit

image::not_R/reg-models.jpg[]

Generalized linear model
------------------------

rfunc[glm] a general solution

* less restrictive requirements on data characteristics
* can change defaults to handle other error distributions

Overkill for many real-life problems: who cares

* uses more cpu time than rfunc[lm]
* uses more memory than rfunc[lm]
* maths is more complicated

FreeBSD sloc over time
----------------------

.Total lines of source code in FreeBSD by days elapsed since the project started.
image::regression-1-BSD-x.jpg[]


Groovy compiler classes over time
---------------------------------

.Number of classes in the Groovy compiler at each release, in days since version 1.0.
image::regression-1-Groovy_change-prop2.jpg[]


How it works
------------

Linear model: simplest form of regression

<equ>y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon</equ>

{nbsp}

response variable, <equ>y</equ>, modeled as a linear combination of

* explanatory variables, <equ>x_n</equ> (prediction variable or
predictor are terms used when emphasizing prediction)

* additive error, <equ>\epsilon</equ>, behavior not accounted for by
explanatory variables, assumed to be independent, identically
distributed

Linear regression
-----------------

Linear refers to the model parameters, i.e., <equ>\beta</equ>

* <equ>y = \alpha + \beta x^2 + \epsilon</equ>
* <equ>y = \alpha + \beta \log(x) + \epsilon</equ>

The most commonly used regression model

* many real world problems exhibit linear behavior, or a good enough
approximation to it for practical purposes, over their input range

* much easier to fit manually than more sophisticated models

* until recently software to build other kinds of models not widely
available

* can generally be built with minimal input from the user

Basic technicalities
--------------------

{nbsp}

The equation:

<equ>E[\mathit{sloc}\] = \alpha + \beta\times\mathit{Number{_}days}</equ>

{nbsp}

is coded as:

[source,R]
-----
BSD_mod=glm(sloc ~ Number_days, data=bsd_info)
-----

Fitted model details
--------------------

[source,R]
-----
summary(BSD_mod)
-----

----
Call:
glm(formula = sloc ~ Number_days, data = kind_bsd)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-82990  -32136   -3609   35389   87324  

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 1.139e+05  1.171e+03   97.24   <2e-16 ***
Number_days 3.937e+02  4.205e-01  936.33   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 1657283104)

    Null deviance: 1.4610e+15  on 4826  degrees of freedom
Residual deviance: 7.9964e+12  on 4825  degrees of freedom
AIC: 116172

Number of Fisher Scoring iterations: 2
----

In mathematics
--------------

The fitted equation is:

<equ>\mathit{sloc} = 1.139\times10^5 + 3.937\times10^2 \mathit{Number{_}days}</equ>

Not a perfect fit between the model and data

* Uncertainty in the values of the model parameters:
+
<equ>\mathit{sloc}  =  (1.139\times10^5\pm1.171\times10^3) + </equ>
<equ>\qquad \qquad \qquad (3.937\times10^2\pm4.205\times10^{-1}) \mathit{Number{_}days}</equ>
////////////
<equ>\begin{eqnarray}
\mathit{sloc} & = & (1.139\times10^5\pm1.171\times10^3) + \\
              &   &   (3.937\times10^2\pm4.205\times10^{-1}) \mathit{Number{_}days}
\end{eqnarray}</equ>
////////////

* Uncertainty caused by the inability of the explanatory variable
used in the model to explain everything:
+
<equ>\mathit{sloc} = 1.139\times10^5 + 3.937\times10^2 \mathit{Number{_}days}</equ>
<equ>\qquad \qquad \qquad	       \pm4.071\times10^4</equ>
////////////
<equ>\begin{eqnarray}
\mathit{sloc} = 1.139\times10^5 & + & 3.937\times10^2 \mathit{Number{_}days} \\
				&   &    \pm4.071\times10^4
\end{eqnarray}</equ>
////////////

Using a model to predict
------------------------

{nbsp}

[source,R]
-----
BSD_pred=predict(BSD_mod)
# or
BSD_pred=predict(BSD_mod,
		 newdata=list(Number_days=0:4000))

lines(BSD_pred, col="red")
-----

The BSD data
------------

.Total lines of source code in FreeBSD by days elapsed since the project started.
image::regression-1-BSD-x.jpg[]


Scattered measurement points
----------------------------

.Estimated cost and duration of 73 large Dutch federal IT projects.
image::regression-1-federal_IT-cost_dur.jpg[]


Confidence intervals
--------------------

{nbsp}

What is the actual fitted model likely to be?

[source,R]
-----
fed_pred=predict(fed_mod, newdata=list(log.IT=1:7),
					se.fit=TRUE)
lines(fed_pred$fit, col="red")

# 2.5% above and below the fit
lines(fed_pred$fit+qnorm(0.975)*fed_pred$se.fit,
					col="green")
lines(fed_pred$fit+qnorm(0.025)*fed_pred$se.fit,
					col="green")

# qnorm(0.975) == 1.96   qnorm(0.025) == -1.96
-----

Prediction intervals
--------------------

{nbsp}

What is the likely value of a new measurement?

{nbsp}

Two sources of uncertainty need to be combined

* uncertainty in the model parameters (i.e., the confidence interval)

* variance in the data not explained by the fitted model

[source,R]
-----
MSE=sum(fed_mod$residuals^2)/(length(fed_mod$residuals)-2)

pred_se=sqrt(fed_pred$se.fit^2+MSE)


lines(fed_pred$fit+1.96*pred_se, col="blue")
lines(fed_pred$fit-1.96*pred_se, col="blue")
-----

Visualizing general trend
-------------------------

.For each distinct language, the number of lines committed on Github and the number of questions tagged with that language.
image::regression-1-langpop-corger-nl.jpg[]


.Percentage of vulnerabilities detected by developers working a given number of years in security.
image::regression-1-acc-sec-experience.jpg[]


Almost any data can be fitted
-----------------------------

.Number of updates and fixes in each Linux release between version 2.6.11 and 3.2.
image::regression-1-linux-stable.jpg[]


rexample[linux-stable]

Updates rfunc[summary] output
-----------------------------

<rcode>Pr(>|t|)</rcode> the p-value for the hypothesis that the
coefficient in the corresponding row is zero,

----
Call:
glm(formula = Fixes ~ Total.Updates, data = cleaned)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-310.60  -223.67     0.48   184.51   525.26  

Coefficients:
              Estimate Std. Error t value Pr(>|t|)   
(Intercept)    356.233    101.522   3.509   0.0016 **
Total.Updates   -4.464      8.478  -0.526   0.6029   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 60685.71)

    Null deviance: 1655335  on 28  degrees of freedom
Residual deviance: 1638514  on 27  degrees of freedom
AIC: 405.62

Number of Fisher Scoring iterations: 2
----

Nominal data
------------

Impact of compiler optimization flags on ability to continue
operating correctly when subject to random bit-flips

Fitted model has:

* response variable: percentage of correct benchmark program execution
* explanatory variable: optimization level as the explanatory variable

Call to rfunc[glm]:

[source,R]
-----
bitflip_mod=glm(pass.masked ~ opt_level, data=bitflip)
-----

rfunc[summary] output
---------------------

----
Call:
glm(formula = pass.masked ~ opt_level, data = bitflip)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-12.6689   -2.8454   -0.3478    4.4017    8.1100  

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)    28.589      1.825  15.665  < 2e-16 ***
opt_levelO2     9.161      2.581   3.550  0.00112 ** 
opt_levelO3     7.429      2.581   2.878  0.00677 ** 
opt_levelosf   11.642      2.414   4.822 2.74e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 29.97578)

    Null deviance: 1785.8  on 38  degrees of freedom
Residual deviance: 1049.2  on 35  degrees of freedom
AIC: 249.07

Number of Fisher Scoring iterations: 2
----

In mathematics
--------------

Fitted equation is:

<equ>\mathit{pass.masked} = 28.6 + 9.2\times D_1 + 7.4\times D_2 + 11.6\times D_3</equ>

<equ>D_i</equ> is known as dummy variables or indicator variable and
has one of two values:

<equ>\qquad \qquad 1\quad\mathrm{optimization flag used} </equ>

<equ>D  = </equ>

<equ>\qquad \qquad 0\quad\mathrm{optimization flag not used} </equ>
//////////
<equ>\begin{eqnarray}
  &   &1\quad\mathrm{optimization flag used}\\
D & = & \\
  &   &0\quad\mathrm{optimization flag not used}
\end{eqnarray}</equ>
//////////

<roption>O0</roption> value is implicit

Standard errors in <roption>O2</roption>/<roption>O3</roption> large
enough for their respective <routput>Estimate</routput>s to overlap

Curved data
-----------

[source,R]
-----
linux_mod=glm(sloc ~ Number_days,
					data=linux_info)
linux_mod=glm(sloc ~ Number_days+I(Number_days^2),
					data=linux_info)
-----

How much better is a quadratic equation fit, a cubic fit, ...?

.Lines of code in every initial release of the Linux kernel since version 1.0.
image::regression-1-linux-DAYLOC.jpg[]


AIC
---

AIC Akaiki Information criteria takes into account:

* a measure of how well a model fits the data

* free parameters have to pay their way by providing enough
improvement in a model's fit to the data

----
[1] Degree 1, AIC= 13998.0004739753
[1] Degree 2, AIC= 13674.6883243397
[1] Degree 3, AIC= 13220.8542892188
[1] Degree 4, AIC= 13221.7072389496
----

When AIC of two models differ by:

* less than 2: more or less equivalent

* between 4 and 7: clearly distinguishable

* more than 10: definitely different

Cubic model
-----------

----
Call:
glm(formula = LOC ~ Number_days + I(Number_days^2) + I(Number_days^3), 
    data = latest_version)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-428217   -80061     6503    64889   620500  

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)       3.432e+05  2.876e+04  11.935  < 2e-16 ***
Number_days      -3.664e+02  5.144e+01  -7.123 3.79e-12 ***
I(Number_days^2)  8.167e-01  2.456e-02  33.258  < 2e-16 ***
I(Number_days^3) -9.184e-05  3.371e-06 -27.242  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 23001633959)

    Null deviance: 1.8867e+15  on 494  degrees of freedom
Residual deviance: 1.1294e+13  on 491  degrees of freedom
AIC: 13221

Number of Fisher Scoring iterations: 2
----

Influential observations and Outliers
-------------------------------------

.Hours to develop software for 29 embedded consumer products and the amount of code they contain.
image::regression-1-lines-per-hour.jpg[]


Ignoring sample size
--------------------

.After removal of overly influential observations.  Left takes into account the Bonferroni p-value of the Studentized residuals while the right plot does not.
image::regression-1-lines-hour-mininf.jpg[]


influenceIndexPlot
------------------

.rfunc[influenceIndexPlot] for the data.
image::regression-1-lines-influence.jpg[]


Outlier or changepoint?
-----------------------

.Number of medical devices reported recalled by the US Food and Drug Administration, in two week bins.  Fitted straight regression line (red, with blue confidence bounds) and loess fit (green).
image::regression-1-Alemzadeh-Recalls.jpg[]


After 2010 is different
-----------------------

.Fitted model after two observations replaced by mean value (left) and two fitted lines, one up to the end of 2010 and one after 2010.
image::regression-1-Alemzadeh-mininf.jpg[]


Multiple regression
-------------------

{nbsp}

Multiple - more than one explanatory variable:

{nbsp}

<equ>y = \alpha + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon</equ>

SPECint 2006
------------

.SPECint 2006 performance results for processors running at various clock rates, memory chip frequencies and processor family.
image::regression-1-SPEC-Mhz.jpg[]


The more the merrier
--------------------

{nbsp}

SPECint results contain 36 columns

R formula notation

* can specify all columns in the sample as explanatory variables

[source,R]
-----
spec_mod=glm(Result ~ ., data=cint)
-----

Information in <rcode>Processor</rcode> column to create an almost
perfect model

Just the important variables
----------------------------

80% of the variance in the data is explained by:

[source,R]
-----
spec_mod=glm(Result ~ Processor.MHz + mem_rate + mem_freq,
					        data=cint)
-----

<rcode>+</rcode> operator specifies that explanatory variables are
added

Equation fitted:

<equ>\mathit{Result}=-2.4\times 10^1+\mathit{Processor.MHz}7.3\times 10^{-3}+</equ>
<equ>\qquad \qquad \qquad \mathit{mem{_}rate}2.5\times 10^{-3} + \mathit{mem{_}freq}1.0\times 10^{-2}</equ>

//////////
<equ>\mathit{Result}=-2.4\times 10^1+\mathit{Processor.MHz}7.3\times 10^{-3}+\mathit{mem{_}rate}2.5\times 10^{-3}+\mathit{mem{_}freq}1.0\times 10^{-2}</equ>
//////////

SPEC summary output
-------------------

----
Call:
glm(formula = Result ~ Processor.MHz + mem_rate + mem_freq, data = cint)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-15.4056   -2.4448    0.0914    2.3792   11.9429  

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)   -2.761e+01  8.388e-01  -32.92   <2e-16 ***
Processor.MHz  7.282e-03  2.496e-04   29.18   <2e-16 ***
mem_rate       2.453e-03  3.912e-05   62.70   <2e-16 ***
mem_freq       1.363e-02  5.524e-04   24.68   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for gaussian family taken to be 14.53335)

    Null deviance: 109700  on 1345  degrees of freedom
Residual deviance:  19504  on 1342  degrees of freedom
AIC: 7428.3

Number of Fisher Scoring iterations: 2
----

Individual variable contribution
--------------------------------

.Individual contribution of each explanatory variable to the response variable in the model of SPECint performance fitted above.
image::regression-1-SPEC-ind-MHz-DDR.jpg[]


Residuals
---------

.Residual of the model of SPECint performance fitted above.
image::regression-1-SPEC-resid-MHz-DDR.jpg[]


Quadratic SPEC model
--------------------

[source,R]
-----
spec_mod=glm(Result ~ Processor.MHz + I(Processor.MHz^2)+
			mem_rate + I(mem_rate^2)+
						mem_freq
                        , data=cint)
-----

.Individual contribution of each explanatory variable to the response variable in a quadratic model of SPECint performance.
image::regression-1-SPEC-quad-MHz-DDR.jpg[]


Updating fitted models
------------------------

[source,R]
-----
ecc_spec_mod=update(spec_mod, . ~ . + ecc)
-----

Data to try
-----------

Formula notation
----------------

.Symbols that can be used within a formula to express relationships between explanatory variables.
[label="feat-mean-var-skew", width="75%", cols="^s,<6d"]
[options="header", frame="topbot", grid="none", align="center"]
|===============================
|Operator| Effect
|+| causes both of its operands to be included in the equation.
|:| denotes an interaction between its operands, e.g., <rcode>a:b</rcode> or  <rcode>a:b:c</rcode>.
|*| denotes all possible combinations of <rcode>+</rcode> and <rcode>:</rcode> operators, e.g., <rcode>a*b</rcode> is equivalent to  <rcode>a+b+a:b</rcode>.
|^| denotes all interactions to a specific degree, e.g., <rcode>(a+b+c)^2</rcode> is equivalent to  <rcode>a+b+c+a:b+a:c+b:c</rcode>.
|.| denotes all variables in the data-frame specified in the <roption>data</roption> argument except the response variable.
|-| specifies that the right operand is removed from the equation, e.g., <rcode>a*b-a</rcode> is equivalent to  <rcode>b+a:b</rcode>.
|-1| specifies that an intercept is not to be fitted (many regression fitting functions implicitly include an intercept)
|I()| ...
|===============================

Interaction and shared contribution
-----------------------------------

.Diagram illustrating shared and non-shared contributions made by two variables in explaining Y.
image::regression-1-shared-contrib.jpg[]


Two models or one?
------------------

.Estimated and actual effort broken down by communication frequency, along with individually straight fitted lines.
image::regression-1-simula_0a.jpg[]


rexample[simula_0a.R]

Interacting variables
---------------------

Simultaneously fit two straight lines to the data

[source,R]
-----
sim_mod=glm(Actual ~ Estimated+Estimated:Communication,
						data=sim)
-----

Fitted equation:

<equ>\mathit{Actual} = -270.1+1.18\mathit{Estimated}+0.51\mathit{Estimated}\times D </equ>
<equ>\qquad \qquad =  -270.1+(1.18+0.51 D)\mathit{Estimated}</equ>

<equ>\qquad \qquad 1\quad\mathrm{daily communication} </equ>

<equ>D  = </equ>

<equ>\qquad \qquad 0\quad\mathrm{not daily communication} </equ>

//////////
<equ>\begin{aligned}
\mathit{Actual} & = & -270.1+1.18\mathit{Estimated}+0.51\mathit{Estimated}\times D \\
                & = & -270.1+(1.18+0.51 D)\mathit{Estimated}
\end{aligned}</equ>

<equ>D = \begin{cases}1& \text{daily communication}\\0& \text{not daily communication}\end{cases}</equ>
//////////

Start complicated
-----------------

[source,R]
-----
sim_mod=glm(Actual ~ (Estimated+Communication+
				Contract+Complexity)^2,
						data=sim)
min_sim=stepAIC(sim_mod)
Anova(min_sim)
-----

----
Analysis of Deviance Table (Type II tests)

Response: Actual
                        LR Chisq Df Pr(>Chisq)    
Estimated                 45.879  1  1.258e-11 ***
Communication             17.272  1  3.240e-05 ***
Contract                   6.767  3    0.07971 .  
Complexity                 1.543  1    0.21423    
Estimated:Communication    2.546  1    0.11060    
Communication:Contract     5.020  3    0.17034    
Contract:Complexity        3.197  2    0.20224    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
----

After following the numbers
---------------------------

[source,R]
-----
sim_mod=glm(Actual ~ Estimated+Communication+
				Communication:Contract,
					data=sim)
-----

Fitted equation is:

<equ>\mathit{Actual} = -274.8+1.21\mathit{Estimated} + 2625\times !D +</equ>
<equ>\qquad \qquad \qquad \qquad C_{fp}(1862\times !D - 197.6\times D) +</equ>
<equ>\qquad \qquad \qquad \qquad C_{tp}(-2270\times !D - 462.2\times D) +</equ>
<equ>\qquad \qquad \qquad \qquad C_{ot}(-2298\times !D - 234.3\times D)</equ>
////////////////
<equ>\begin{aligned}
\mathit{Actual} = -274.8+1.21\mathit{Estimated} + 2625\times !D & + & \\
					&   & C_{fp}(1862\times !D - 197.6\times D) + \\
					&   & C_{tp}(-2270\times !D - 462.2\times D) + \\
					&   & C_{ot}(-2298\times !D - 234.3\times D)
\end{aligned}</equ>
////////////////

* <equ>C_{fp}</equ> 0/1 is a fixed price contract
* <equ>C_{tp}</equ> 0/1 is a target price contract
* <equ>C_{ot}</equ> 0/1 other kind of contract

Data to try
-----------

